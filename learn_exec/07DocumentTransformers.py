
"##################################"
# CodeTextSplitter å…è®¸æ‚¨ä½¿ç”¨å¤šç§è¯­è¨€æ”¯æŒæ‹†åˆ†ä»£ç ã€‚å¯¼å…¥æšä¸¾ Language å¹¶æŒ‡å®šè¯­è¨€ã€‚
from langchain.text_splitter import (
    RecursiveCharacterTextSplitter,
    Language,
)
# æ”¯æŒæ‹†åˆ†çš„è¯­è¨€
print([e.value for e in Language])
"['cpp', 'go', 'java', 'kotlin', 'js', 'ts', 'php', 'proto', 'python', 'rst', 'ruby', 'rust', 'scala', 'swift', 'markdown', 'latex', 'html', 'sol', 'csharp', 'cobol']"

# You can also see the separators used for a given language
print(RecursiveCharacterTextSplitter.get_separators_for_language(Language.PYTHON))

#ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨ PythonTextSplitter çš„ç¤ºä¾‹
PYTHON_CODE = """
def hello_world():
    print("Hello, World!")

# Call the function
hello_world()
"""
python_splitter = RecursiveCharacterTextSplitter.from_language(
    language=Language.PYTHON, chunk_size=50, chunk_overlap=0
)
python_docs = python_splitter.create_documents([PYTHON_CODE])
print(python_docs)

"==========================================================="
# This is a long document we can split up.
with open('documentstore/state_of_the_union.txt') as f:
    state_of_the_union = f.read()

from langchain.text_splitter import RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter(
    # Set a really small chunk size, just to show.
    chunk_size = 100,
    chunk_overlap  = 20,
    length_function = len,
    add_start_index = True,
)
texts = text_splitter.create_documents([state_of_the_union])
print(texts[0])
print(texts[1])
"==========================================================="

#ä¸‹é¢æ˜¯ä½¿ç”¨ Markdown æ–‡æœ¬æ‹†åˆ†å™¨çš„ç¤ºä¾‹ã€‚
markdown_text = """
# ğŸ¦œï¸ğŸ”— LangChain

âš¡ Building applications with LLMs through composability âš¡

## Quick Install

```bash
# Hopefully this code block isn't split
pip install langchain
```

As an open source project in a rapidly developing field, we are extremely open to contributions.
"""
md_splitter = RecursiveCharacterTextSplitter.from_language(
    language=Language.MARKDOWN, chunk_size=60, chunk_overlap=0
)
md_docs = md_splitter.create_documents([markdown_text])
print(md_docs)

"==========================================================="
# æ–‡æ¡£ç›¸å…³æ€§æ£€ç´¢
# å¯¼å…¥å¿…è¦çš„åº“å’Œæ¨¡å—
import os
import chromadb
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.document_transformers import LongContextReorder
from langchain.chains import StuffDocumentsChain, LLMChain
from langchain.prompts import PromptTemplate
from langchain.llms import OpenAI

# è·å–é¢„è®­ç»ƒçš„è¯åµŒå…¥
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

# å®šä¹‰ä¸€ç»„æ–‡æœ¬æ–‡æ¡£
texts = [
    "Basquetball is a great sport.",
    "Fly me to the moon is one of my favourite songs.",
    "The Celtics are my favourite team.",
    "This is a document about the Boston Celtics",
    "I simply love going to the movies",
    "The Boston Celtics won the game by 20 points",
    "This is just a random text.",
    "Elden Ring is one of the best games in the last 15 years.",
    "L. Kornet is one of the best Celtics players.",
    "Larry Bird was an iconic NBA player.",
]

# ä½¿ç”¨è¿™äº›æ–‡æœ¬æ–‡æ¡£å’Œé¢„è®­ç»ƒçš„è¯åµŒå…¥åˆ›å»ºä¸€ä¸ªæ£€ç´¢å™¨
retriever = Chroma.from_texts(texts, embedding=embeddings).as_retriever(
    search_kwargs={"k": 10}
)

# å®šä¹‰ä¸€ä¸ªæŸ¥è¯¢
query = "What can you tell me about the Celtics?"

# åŸºäºè¿™ä¸ªæŸ¥è¯¢æ£€ç´¢ç›¸å…³çš„æ–‡æ¡£ï¼Œå¹¶æŒ‰ç›¸å…³æ€§åˆ†æ•°æ’åº
docs = retriever.get_relevant_documents(query)
print(docs)
